{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lu6ateAjOC6j"
   },
   "source": [
    "# ML in Cybersecurity: Project II\n",
    "\n",
    "## Team\n",
    "  * **Team name**: *Good Fellows*\n",
    "  * **Members**:  *fill this in. format: Md. Abdul Kadir (s8mdkadi@stud.uni-saarland.de), Hasan Md Tusfiqur Alam (s8haalam@stud.uni-saarland.de), ...*\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 28th November 2019, 13:59:59 (right before the lecture)\n",
    "  * Email the completed notebook to: `mlcysec_ws1920_staff@lists.cispa.saarland`\n",
    "  * Complete this in **teams of 3**\n",
    "  * Feel free to use the course [mailing list](https://lists.cispa.saarland/listinfo/mlcysec_ws1920_stud) to discuss.\n",
    "  \n",
    "## Timeline\n",
    "  * 14-Nov-2019: Project 2 hand-out\n",
    "  * **28-Nov-2019** (13:59:59): Email completed notebook\n",
    "  * 5-Nov-2019: Project 2 discussion and summary\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, you will explore an application of ML to a popular task in cybersecurity: malware classification.\n",
    "You will be presented with precomputed behaviour analysis reports of thousands of program binaries, many of which are malwares.\n",
    "Your goal will be train a malware detector using this behavioural reports.\n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The grading for this project will depend on:\n",
    " 1. Vectorizing Inputs\n",
    "   * Obtaining a reasonable vectorized representations of the input data (a file containing a sequence of system calls)\n",
    "   * Understanding the influence these representations have on your model\n",
    " 1. Classification Model  \n",
    "   * Following a clear ML pipeline\n",
    "   * Obtaining reasonable performances (>60\\%) on held-out test set\n",
    "   * Choice of evaluation metric\n",
    "   * Visualizing loss/accuracy curves\n",
    " 1. Analysis\n",
    "   * Which methods (input representations/ML models) work better than the rest and why?\n",
    "   * Which hyper-parameters and design-choices were important in each of your methods?\n",
    "   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n",
    "   * Trade-offs between methods, hyper-parameters, design-choices\n",
    "   * Anything else you find interesting (this part is open-ended)\n",
    "\n",
    "\n",
    "## Grading Details\n",
    " * 40 points: Vectorizing input data (each input = behaviour analysis file in our case)\n",
    " * 40 points: Training a classification model\n",
    " * 15 points: Analysis/Discussion\n",
    " * 5 points: Clean code\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "\n",
    "**The notebook is your project report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use final summary to provide report about these codes.**\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v1.1: Updated deadline\n",
    "  * v1.0: Initial notebook\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7H8Ti01XP79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: livelossplot in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: matplotlib in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from livelossplot) (3.0.3)\n",
      "Requirement already satisfied: notebook in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from livelossplot) (5.7.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from matplotlib->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from matplotlib->livelossplot) (1.16.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from matplotlib->livelossplot) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from matplotlib->livelossplot) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from matplotlib->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (18.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (0.8.2)\n",
      "Requirement already satisfied: ipykernel in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (5.1.0)\n",
      "Requirement already satisfied: nbformat in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (4.3.2)\n",
      "Requirement already satisfied: nbconvert in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (5.5.0)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (2.10.1)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from notebook->livelossplot) (5.2.4)\n",
      "Requirement already satisfied: setuptools in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot) (40.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib->livelossplot) (1.12.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from terminado>=0.8.1->notebook->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipykernel->notebook->livelossplot) (7.5.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbformat->notebook->livelossplot) (3.0.1)\n",
      "Requirement already satisfied: decorator in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from traitlets>=4.2.1->notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (0.3)\n",
      "Requirement already satisfied: bleach in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (3.1.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (0.5.0)\n",
      "Requirement already satisfied: pygments in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (2.3.1)\n",
      "Requirement already satisfied: testpath in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from nbconvert->notebook->livelossplot) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from jinja2->notebook->livelossplot) (1.1.1)\n",
      "Requirement already satisfied: backcall in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.13.3)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (2.0.9)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->livelossplot) (0.15.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->livelossplot) (19.1.0)\n",
      "Requirement already satisfied: webencodings in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from bleach->nbconvert->notebook->livelossplot) (0.5.1)\n",
      "Requirement already satisfied: parso>=0.3.0 in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.4.0)\n",
      "Requirement already satisfied: wcwidth in /Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook->livelossplot) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKz2D08oOC6k"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import itertools  \n",
    "import collections \n",
    "from IPython.display import display, HTML\n",
    "from livelossplot import PlotLosses\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJIYI1VAOC6p"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFZ9qKdmOC6s"
   },
   "outputs": [],
   "source": [
    "compute_mode = 'cpu'\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNGbPB_KOC6v"
   },
   "source": [
    "# Setup\n",
    "\n",
    "  * Download the datasets: [train](https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm) (128M) and [test](https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE) (92M)\n",
    "  * Unpack them under `./data/train` and `./data/test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "SW_h3vQwOC6v",
    "outputId": "777b857c-368b-43bf-80c4-e0d04490bb83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train examples (Should be 13682) :    13682\n",
      "# test  examples (Should be 10000) :    10000\n"
     ]
    }
   ],
   "source": [
    "# Check that you are prepared with the data\n",
    "! printf '# train examples (Should be 13682) : '; ls data/train | wc -l\n",
    "! printf '# test  examples (Should be 10000) : '; ls data/test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62wapp-4OC6y"
   },
   "source": [
    "Now that you're set, let's briefly look at the data you have been handed.\n",
    "Each file encodes the behavior report of a program (potentially a malware), using an encoding scheme called \"The Malware Instruction Set\" (MIST for short).\n",
    "At this point, we highly recommend you briefly read-up Sec. 2 of the [MIST](http://www.mlsec.org/malheur/docs/mist-tr.pdf) documentation.\n",
    "\n",
    "You will find each file named as `filename.<malwarename>`:\n",
    "```\n",
    "» ls data/train | head\n",
    "00005ecc06ae3e489042e979717bb1455f17ac9d.NothingFound\n",
    "0008e3d188483aeae0de62d8d3a1479bd63ed8c9.Basun\n",
    "000d2eea77ee037b7ef99586eb2f1433991baca9.Patched\n",
    "000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "0010f78d3ffee61101068a0722e09a98959a5f2c.Basun\n",
    "0013cd0a8febd88bfc4333e20486bd1a9816fcbf.Basun\n",
    "0014aca72eb88a7f20fce5a4e000c1f7fff4958a.Texel\n",
    "001ffc75f24a0ae63a7033a01b8152ba371f6154.Texel\n",
    "0022d6ba67d556b931e3ab26abcd7490393703c4.Basun\n",
    "0028c307a125cf0fdc97d7a1ffce118c6e560a70.Swizzor\n",
    "...\n",
    "```\n",
    "and within each file, you will see a sequence of individual systems calls monitored duing the run-time of the binary - a malware named 'Basun' in the case:\n",
    "```\n",
    "» head data/train/000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "# process 000006c8 0000066a 022c82f4 00000000 thread 0001 #\n",
    "02 01 | 000006c8 0000066a 00015000\n",
    "02 02 | 00006b2c 047c8042 000b9000\n",
    "02 02 | 00006b2c 047c8042 00108000\n",
    "02 02 | 00006b2c 047c8042 00153000\n",
    "02 02 | 00006b2c 047c8042 00091000\n",
    "02 02 | 00006b2c 047c8042 00049000\n",
    "02 02 | 00006b2c 047c8042 000aa000\n",
    "02 02 | 00006b2c 047c8042 00092000\n",
    "02 02 | 00006b2c 047c8042 00011000\n",
    "...\n",
    "```\n",
    "(**Note**: Please ignore the first line that begins with `# process ...`.)\n",
    "\n",
    "Your task in this project is to train a malware detector, which given the sequence of system calls (in the MIST-formatted file like above), predicts one of 10 classes: `{ Agent, Allaple, AutoIt, Basun, NothingFound, Patched, Swizzor, Texel, VB, Virut }`, where `NothingFound` roughly represents no malware is present.\n",
    "In terms of machine learning terminology, your malware detector $F: X \\rightarrow Y$ should learn a mapping from the MIST-encoded behaviour report (the input $x \\in X$) to the malware class $y \\in Y$.\n",
    "\n",
    "Consequently, you will primarily tackle two challenges in this project:\n",
    "  1. \"Vectorizing\" the input data i.e., representing each input (file) as a tensor\n",
    "  1. Training an ML model\n",
    "  \n",
    "\n",
    "### Some tips:\n",
    "  * Begin with an extremely simple representation/ML model and get above chance-level classification performance\n",
    "  * Choose your evaluation metric wisely\n",
    "  * Save intermediate computations (e.g., a token to index mapping). This will avoid you parsing the entire dataset for every experiment\n",
    "  * Try using `multiprocessing.Pool` to parallelize your `for` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeZ8RIwsOC6y"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5LTm8brnOC6z"
   },
   "source": [
    "# 1. Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32b2VS6YOC60"
   },
   "source": [
    "## 1.a. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EjFXD90uOC60"
   },
   "outputs": [],
   "source": [
    "def file_load_kernel(filepath):\n",
    "    '''Given a filepath, returns (content, classname), where content = [list of lines in file]'''\n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    # Parse the file name and identify the class name.\n",
    "    label = filepath.split('.')[2]\n",
    "    \n",
    "    # Read the file \n",
    "    with open(filepath, 'r') as file :\n",
    "        lines = file.read().splitlines()\n",
    "    if '# process' in lines[0] :\n",
    "        del lines[0]\n",
    "    \n",
    "    \n",
    "    # Remove the firstline start with \"# process\"\n",
    "    \n",
    "    # Convert text file to list , content of list is the lines of the textfile.\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lines, label\n",
    "\n",
    "\n",
    "def handler(function, data, workers=6):\n",
    "    \n",
    "    with Pool(workers) as pool:\n",
    "        #pool.map(cycle, offsets)\n",
    "        r=pool.map(function, data)\n",
    "    return r\n",
    "\n",
    "def load_data(data_path, nworkers=6):\n",
    "    '''Returns each data sample as a tuple (x, y), x = sequence of strings (i.e., syscalls), y = malware program class'''\n",
    "    raw_data_samples = []\n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #\n",
    "    #\n",
    "    files = os.listdir(data_path)\n",
    "    # Removing the empty index\n",
    "    file_addresses = []\n",
    "    del files[-1]\n",
    "    i = 1\n",
    "    for file in files:\n",
    "        file_addresses.append (os.path.join(data_path,file))\n",
    "        #i += 1\n",
    "        #if i> 200:\n",
    "         #   break #comment this to load whole dataset\n",
    "        \n",
    "    print(file_addresses)\n",
    "    raw_data_samples = handler(file_load_kernel,file_addresses,workers=4)\n",
    "    return raw_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oq688dQ2OC61"
   },
   "outputs": [],
   "source": [
    "# Test \n",
    "\n",
    "#print(file_load_kernel('./data/train/fffb9be0177ac57f213bc089b319582936a5bd08.NothingFound'))\n",
    "#print(load_data('./data/train', nworkers=4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qtkh-tdFOC63"
   },
   "source": [
    "## 1.b. Vectorize: Setup\n",
    "\n",
    "Make one pass over the inputs to identify relevant features/tokens.\n",
    "\n",
    "Suggestion:\n",
    "  - identify tokens (e.g., unigrams, bigrams)\n",
    "  - create a token -> index (int) mapping. Note that you might have a >10K unique tokens. So, you will have to choose a suitable \"vocabulary\" size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEAwGYIIOC63"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "def countFrequency(my_list): \n",
    "      # This taking long time to read\n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items) \n",
    "    return freq \n",
    "\n",
    "#Use this method to get frequecy table \n",
    "\n",
    "def load_file_to_freq_dict(file_name):\n",
    "    \n",
    "    # Read the file \n",
    "    with open(file_name, 'r') as file :\n",
    "        lines = file.read().splitlines()\n",
    "    if '# process' in lines[0] :\n",
    "        del lines[0]\n",
    "    freq = countFrequency(lines)\n",
    "    return freq\n",
    "\n",
    "# it use multiple files to create a frequency counter\n",
    "def load_multiple_files_frequecy_list(file_list):\n",
    "    list_of_dict = handler(load_file_to_freq_dict, file_list, workers=4)\n",
    "    print('multiprocess finished')\n",
    "    return  {k: v for d in list_of_dict for k, v in d.items()}\n",
    "    \n",
    "\n",
    "def get_key_idx_map(counter, vocab_size, ukn_token='_ukn_'):\n",
    "    # counter is a mapping: token -> count\n",
    "    # build vectorizer using vocab_size most common elements\n",
    "    counter_unk = counter\n",
    "    idx_to_key = {}\n",
    "    key_to_idx = {}\n",
    "    \n",
    "    \n",
    "    if len(counter)< vocab_size:\n",
    "        counter_unk.update({ukn_token:(vocab_size - len(counter))})\n",
    "        \n",
    "    counter_unk = dict(sorted(counter_unk.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    for i in range(len(counter_unk)):\n",
    "        idx_to_key.update({i:list(counter_unk.keys())[i]})\n",
    "        key_to_idx.update({list(counter_unk.keys())[i]:i})\n",
    "    \n",
    "    #key_to_idx, idx_to_key = dict(), dict()\n",
    "    return key_to_idx, idx_to_key\n",
    "# Helper class to get all text files in a list\n",
    "def get_all_file_address(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files = [os.path.join(directory,file) for file in files]\n",
    "    \n",
    "    return files\n",
    "# helper class to get token count\n",
    "\n",
    "    \n",
    "\n",
    "def get_class_key_index_map(class_counter):\n",
    "  #  { 'Agent':0, 'Allaple':1, 'AutoIt': 2, 'Basun':3, 'NothingFound':4, 'Patched': 5, 'Swizzor':6, 'Texel':7, 'VB': 8, 'Virut':9 }\n",
    "    class_index = class_counter\n",
    "    index_class = dict((v,k) for k,v in class_index.items())\n",
    "    nb_classes = len(class_counter)\n",
    "    targets = np.array([class_counter.keys()]).reshape(-1)\n",
    "    one_hot_targets = np.eye(nb_classes).tolist()\n",
    "    class_index = dict(zip([*class_index], one_hot_targets))\n",
    "    return class_index, index_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm2k0ZNHOC64"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "#my_map = {'a': 1, 'b': 2}\n",
    "#print(get_key_idx_map(my_map,4))\n",
    "classes = { 'Agent':0, 'Allaple':1, 'AutoIt': 2, 'Basun':3, 'NothingFound':4, 'Patched': 5, 'Swizzor':6, 'Texel':7, 'VB': 8, 'Virut':9 }\n",
    "class_onehot, index_class = get_class_key_index_map(classes)\n",
    "#print(class_onehot)\n",
    "#print(index_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13682\n",
      "2737\n",
      "length of each chunk will oprocessed in pool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-63:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e945e3c37070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#list_dict_count = handler(load_multiple_files_frequecy_list, multiple_training_file_chunk, workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_multiple_files_frequecy_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple_training_file_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcount_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-76f182c58d51>\u001b[0m in \u001b[0;36mload_multiple_files_frequecy_list\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# it use multiple files to create a frequency counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_multiple_files_frequecy_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlist_of_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_file_to_freq_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiprocess finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-a2814ec8b938>\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(function, data, workers)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#pool.map(cycle, offsets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of all files \n",
    "\n",
    "training_files = get_all_file_address('./data/train')\n",
    "print(len(training_files))\n",
    "multiple_training_file_chunk = [training_files[x:x+5] for x in range(0, len(training_files),5)]\n",
    "print(len(multiple_training_file_chunk))\n",
    "\n",
    "print(\"length of each chunk will oprocessed in pool\", len(multiple_training_file_chunk[0]))\n",
    "# Reading token count using multiprocessing handaller \n",
    "#list_dict_count = handler(load_multiple_files_frequecy_list, multiple_training_file_chunk, workers=4)\n",
    "\n",
    "print(load_multiple_files_frequecy_list(multiple_training_file_chunk[0]))\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "#for l in multiple_training_file_chunk:\n",
    "#    count_dict = {**count_dict, **load_multiple_files_frequecy_list(l)}\n",
    "#    print('counting the key')\n",
    "    \n",
    "\n",
    "#print(len(count_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivtsJcF5OC65"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "# Count maximum vocab size\n",
    "\n",
    "# Comment this after a full run. \n",
    "training_data = load_data('./data/train', nworkers=6)\n",
    "    \n",
    "print('training data reading finish')\n",
    "tokens = []\n",
    "for pairs in training_data:\n",
    "    tokens = tokens + pairs[0]\n",
    "\n",
    "token_chunks = [tokens[x:x+1000] for x in range(0, len(tokens), 1000)]\n",
    "\n",
    "print('token generation is done')\n",
    "#print(tokens)\n",
    "MAX_VOCAB_SIZE = len(set(tokens)) + 100 # Assumtion \n",
    "\n",
    "token_counter =  handler(countFrequency,token_chunks)\n",
    "\n",
    "combined_token_dict = {k: v for d in token_counter for k, v in d.items()}\n",
    "\n",
    "#print(token_counter)\n",
    "print('frequency counting is done')\n",
    "print()\n",
    "token_to_idx, idx_to_token = get_key_idx_map(combined_token_dict, MAX_VOCAB_SIZE)    \n",
    "# Save vocab to file\n",
    "\n",
    "out_path = 'application_vocab_{}.pkl'.format(MAX_VOCAB_SIZE)\n",
    "with open(out_path, 'wb') as wf:\n",
    "    dct = {'token_to_idx': token_to_idx,\n",
    "          'idx_to_token': idx_to_token}\n",
    "    pickle.dump(dct, wf)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ERrv9TOC66"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxfFySjROC67"
   },
   "outputs": [],
   "source": [
    "#print(token_to_idx)\n",
    "#print(idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTU5mqJEOC67"
   },
   "source": [
    "## 1.c. Vectorize Data\n",
    "\n",
    "Use the (token $\\rightarrow$ index) mapping you created before to vectorize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVedD8icOC68"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-da9aeb07fe2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data from pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mvectorized_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CHNAGE THE NAME OF FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mvectorizer_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvectorized_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_path' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "# Load the data from pickle file \n",
    "\n",
    "vectorized_file = open(out_path,'rb')  # CHNAGE THE NAME OF FILE\n",
    "vectorizer_data = pickle.load(vectorized_file)\n",
    "vectorized_file.close()\n",
    "print(type(vectorizer_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyYjBuDgOC69"
   },
   "outputs": [],
   "source": [
    "def vectorize_raw_samples(raw_samples, nworkers=4):\n",
    "    vectorized_samples = []\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #\n",
    "    #\n",
    "    token_to_idx = vectorizer_data['token_to_idx']\n",
    "    data = load_data(raw_samples, nworkers)\n",
    "    for l in data:\n",
    "        temp_list = []\n",
    "        for i in l[0]:\n",
    "            index = token_to_idx.get(i)#[token_to_idx[i]]\n",
    "            if index:\n",
    "                temp_list.append(index)\n",
    "            else:\n",
    "                temp_list.append(token_to_idx.get('_ukn_'))\n",
    "        if(class_onehot.get(l[1])):\n",
    "            vectorized_samples.append((temp_list, class_onehot.get(l[1])))\n",
    "        else:\n",
    "            vectorized_samples.append((temp_list, class_onehot.get( 'NothingFound'))) \n",
    "        #vectorized_samples.append(([token_to_idx.get(i) if token_to_idx.get(i) else token_to_idx.get('_ukn_') for  i in l[0]], l[1]))\n",
    "    \n",
    "    \n",
    "    return vectorized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etN5gu8ZZqlJ"
   },
   "outputs": [],
   "source": [
    "def store_as_pickle(file_name, data):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def load_from_pickle(file_name):\n",
    "    with open(file_name, \"rb\") as input_file:\n",
    "        data = pickle.load(input_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZSakyyhOC69",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('=> Processing: Train')\n",
    "train_raw_samples = './data/train'\n",
    "test_raw_samples = './data/test'\n",
    "train_data = vectorize_raw_samples(train_raw_samples) # comment this line after first run\n",
    "print('=> Processing: Test')\n",
    "test_data = vectorize_raw_samples(test_raw_samples) # Comment this line after first run\n",
    "#train_data = load_from_pickle('training_data.pickle')\n",
    "#test_data = load_from_pickle('test_data.pickle')\n",
    "#print(train_data)\n",
    "#print(test_data[0])\n",
    "\n",
    "store_as_pickle('training_data.pickle', train_data)\n",
    "store_as_pickle('test_data.pickle', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5_elKosZoh9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mxNMm9cOC6_"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "def max_list_lenght(list_2d):\n",
    "    max_len = 0\n",
    "    for i in list_2d:\n",
    "        if len(i)> max_len:\n",
    "            max_len = len(i)\n",
    "    return max_len\n",
    "        \n",
    "\n",
    "train_x , train_y = [x for x,_ in train_data],[y for _,y in train_data]\n",
    "test_x , test_y = [x for x,_ in test_data],[y for _,y in test_data]\n",
    "\n",
    "max_len = max_list_lenght(train_x)\n",
    "train_x = [x + [0] * (max_len - len(x)) for x in train_x]\n",
    "#print(train_x)\n",
    "train_x = torch.tensor(train_x)\n",
    "\n",
    "\n",
    "train_y = torch.FloatTensor(train_y)\n",
    "test_x = [x + [0] * (max_len - len(x)) for x in test_x]\n",
    "test_x = torch.FloatTensor(test_x)\n",
    "test_y = torch.FloatTensor(test_y)\n",
    "\n",
    "\n",
    "\n",
    "#print(train_y)\n",
    "\n",
    "# Suggestions: \n",
    "#\n",
    "# (a) You can use torch.utils.data.TensorDataset to represent the tensors you created previously\n",
    "trainset = TensorDataset(train_x, train_y)  \n",
    "testset = TensorDataset(test_x, test_y)\n",
    "#\n",
    "# (b) Store your datasets to disk so that you do not need to precompute it every time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4QyKy0yOC6_"
   },
   "source": [
    "# 2. Train Model\n",
    "\n",
    "You will now train an ML model on the vectorized datasets you created previously.\n",
    "\n",
    "_Note_: Although I often refer to each input as a 'vector' for simplicity, each of your inputs can also be higher dimensional tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8W3fKAHOC7A"
   },
   "source": [
    "## 2.a. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J52bJL_OOC7A"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "def evaluate_preds(y_gt, y_pred):\n",
    "    y_gt = [np.where(r==1)[0][0] for r in y_gt]\n",
    "    y_pred = [np.where(r==1)[0][0] for r in y_pred]\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def another_helper(question):\n",
    "    return 42\n",
    "\n",
    "\n",
    "def save_model(model, out_path):\n",
    "    torch.save(model.state_dict(), out_path)\n",
    "\n",
    "\n",
    "def save_data(eval_data, out_path):\n",
    "    with open(out_path, 'wb') as handle:\n",
    "        pickle.dump(eval_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cwu2cLIOC7B"
   },
   "source": [
    "## 2.b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qpH1SvvOC7B"
   },
   "source": [
    "Describe your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgB7AVdBOC7B"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "class NetDNN(nn.Module):\n",
    "    def __init__(self, in_dims, h1, h2, out_dims):\n",
    "        super(NetDNN, self).__init__()\n",
    "        # Layer definitions\n",
    "        self.fc1 = nn.Linear(in_dims, h1)\n",
    "        self.fc2 = nn.Linear(h1,h2)\n",
    "        self.fc3 = nn.Linear(h2,out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x =  F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x =  F.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lR7yv04OC7C"
   },
   "source": [
    "## 2.c. Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DQTeaoDOC7D"
   },
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "\n",
    "in_dims = trainset[0][0].shape[0]\n",
    "print('Input di',in_dims)\n",
    "h1, h2 = 128, 128\n",
    "out_dims = len(index_class)\n",
    "print('Output dim', out_dims)\n",
    "\n",
    "# Optimization\n",
    "n_epochs = 25\n",
    "batch_size = 32 # Originally 32\n",
    "lr = 1e-2\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE6rO5ppOC7E"
   },
   "source": [
    "## 2.d. Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXjYf7NlOC7F"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "net = NetDNN(in_dims, h1, h2, out_dims)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# Data Loaders\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloader = {'train': trainloader,'val': testloader}\n",
    "\n",
    "# Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNFDuH_5OC7G"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# Example:\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "def train_model(model,criterion, optimizer, n_epochs):\n",
    "    eval_data = {}\n",
    "    model = model.to(device)\n",
    "    live_plot = PlotLosses()\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            if phase == 'val':\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            ruuning_corrects = 0\n",
    "\n",
    "\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs.float())\n",
    "                labels = labels.to(device=device, dtype=torch.int64)\n",
    "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.detach()* inputs.size(0)\n",
    "            labels_nonhot =  torch.tensor([np.where(r==1)[0][0] for r in labels.data])\n",
    "\n",
    "            ruuning_corrects += torch.sum(preds == labels_nonhot)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = ruuning_corrects.float() / len(dataloader[phase].dataset)\n",
    "      \n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log_loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "            \n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(logs['log_loss'])\n",
    "        test_loss_list.append(logs['val_log_loss'])\n",
    "        train_acc_list.append(logs['accuracy'])\n",
    "        test_acc_list.append(logs['val_accuracy'])\n",
    "        live_plot.update(logs)\n",
    "        live_plot.draw()\n",
    "    eval_data['epoch'] = epoch_list\n",
    "    eval_data['train_loss'] = train_loss_list\n",
    "    eval_data['test_loss'] = test_loss_list\n",
    "    eval_data['train_acc'] = train_acc_list\n",
    "    eval_data['test_acc'] = test_acc_list\n",
    "    \n",
    "    return model, eval_data   \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB7i1l1NOC7G"
   },
   "outputs": [],
   "source": [
    "#Print training loss.\n",
    "\n",
    "model, eval_data =  train_model(net, criterion, optimizer,n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo0nrl9OOC7I"
   },
   "source": [
    "## 2.e. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocCgEDC3OC7I"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "features, labels = dataiter.next()\n",
    "outputs = net(features)\n",
    "_ ,outputs = torch.max(outputs, 1)\n",
    "_, labels_nonhot =  torch.max(labels,1)\n",
    "accuracy = 100 * torch.sum(outputs == labels_nonhot) / len(outputs)\n",
    "print('Overall test accuracy(%) : ', accuracy.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5U3uSy5nOC7K"
   },
   "source": [
    "## 2.f. Save Model + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2YHIUXLOC7K"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "expt_name = 'model:mlp_tokenizer:foo_key3:val3_key4:val4'\n",
    "\n",
    "model_out_path = '{}.checkpoint.pth'.format(expt_name)\n",
    "save_model(model, model_out_path)\n",
    "\n",
    "eval_out_path = '{}.eval.pickle'.format(expt_name)\n",
    "print(eval_out_path)\n",
    "save_data(eval_data, eval_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgB3tmIbOC7M"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWuZKTeyOC7M"
   },
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5gBE6SROC7M"
   },
   "source": [
    "## 3.a. Summary: Main Results\n",
    "\n",
    "If you tried other approaches, summarize their results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZ-leib6OC7M"
   },
   "source": [
    "|        | Input Representation | Model | Optimizer | Validation Metric | Test Metric |\n",
    "|--------|----------------------|-------|-----------|-------------------|-------------|\n",
    "| Model1 | Unigram tokens       | MLP   | SGD       | 12.34 %           | 23.45%      |\n",
    "| Model2 (this notebook) |                      |       |           |                   |             |\n",
    "| ...    |                      |       |           |                   |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "t4DVUZC2OC7M"
   },
   "source": [
    "## 3.b. Discussion\n",
    "\n",
    "Enter your final summary here.\n",
    "\n",
    "For instance, you can address:\n",
    "- What was the performance you obtained with the simplest approach?\n",
    "- Which vectorized input representations helped more than the others?\n",
    "- Which malwares are difficult to detect and why?\n",
    "- Which approach do you recommend to perform malware classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5ZWU_POOC7N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjmRUbcLOC7N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
